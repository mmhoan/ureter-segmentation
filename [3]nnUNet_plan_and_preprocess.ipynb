{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uk2/바탕화면/GUAI MOON/nnunet_code\n"
     ]
    }
   ],
   "source": [
    "cd '/home/uk2/바탕화면/GUAI MOON/nnunet_code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##################### nnunet/experiment_planning/nnUNet_plan_and_preprocess.py\n",
    "###############################\n",
    "import nnunet\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from nnunet.experiment_planning.DatasetAnalyzer import DatasetAnalyzer\n",
    "from nnunet.experiment_planning.utils import crop\n",
    "from nnunet.paths import *\n",
    "import shutil\n",
    "from nnunet.utilities.task_name_id_conversion import convert_id_to_task_name\n",
    "from nnunet.preprocessing.sanity_checks import verify_dataset_integrity\n",
    "import easydict\n",
    "from nnunet.training.model_restore import recursive_find_python_class\n",
    "'''\n",
    "task_ids : 4 (=Task004_Hippocampus)\n",
    "planner3d : 3d version\n",
    "planner2d : 2d version\n",
    "no_pp : preprocessing 여부\n",
    "tl(processes_lowres) : low resolution process number\n",
    "tf(processes_fullres) : high resolution process number\n",
    "verify_dataset_integrity : check dataset integrity\n",
    "'''\n",
    "args = easydict.EasyDict({\n",
    "        \"task_ids\" : None,\n",
    "        \"planner3d\" : \"ExperimentPlanner3D_v21_11GB\",\n",
    "        \"planner2d\" : \"ExperimentPlanner2D_v21\",\n",
    "        \"no_pp\" : None,\n",
    "        \"tl\" : 8,\n",
    "        \"tf\" : 8,\n",
    "        \"verify_dataset_integrity\" : False\n",
    "})\n",
    "\n",
    "args.task_ids = ['99']#['19']  # (=Task004_Hippocampus)\n",
    "args.verify_dataset_integrity = True\n",
    "\n",
    "task_ids = args.task_ids\n",
    "dont_run_preprocessing = args.no_pp\n",
    "tl = args.tl\n",
    "tf = args.tf\n",
    "planner_name3d = args.planner3d\n",
    "planner_name2d = args.planner2d\n",
    "\n",
    "tasks = []\n",
    "for i in task_ids:\n",
    "    i = int(i)\n",
    "    # 4 -> Task004_Hippocampus\n",
    "    task_name = convert_id_to_task_name(i)\n",
    "\n",
    "    if args.verify_dataset_integrity:\n",
    "        #verify_dataset_integrity(join(nnUNet_raw_data, task_name))\n",
    "        pass\n",
    "    ##########################################################################################################\n",
    "    # 0인 가장자리 crop, seg special label(nnUNet_cropped_data)\n",
    "    '''\n",
    "    1. 데이터 저장(nnUNet_raw_data -> nnUNet_cropped_data)\n",
    "    1) Segmentation file 저장 : (nnUNet_raw_data -> nnUNet_cropped_data/gt_segmentations)\n",
    "    2) Crop data(original,segmentation) 저장 : (nnUnet_cropped_data/hippocampus_001.npz, hippocampus_002.npz,...)\n",
    "    3) Crop data information 저장 (nnUNet_cropped_data/hippocampus_001.pkl, hippocampus_002.pkl,...)\n",
    "        - properties[original_size_of_raw_data // original_spacing // list_of_data_files // seg_file]\n",
    "        - properties[itk_origin, itk_spacing, itk_direction // crop_bbox // classes // size_after_cropping]\n",
    "    (자세한 tag 정보는 experiment_planning/utils.py(crop) 참조)\n",
    "    '''\n",
    "    crop(task_name, False, tf)\n",
    "    ##########################################################################################################\n",
    "    tasks.append(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_in = join(nnunet.__path__[0], \"experiment_planning\")\n",
    "\n",
    "if planner_name3d is not None:\n",
    "    planner_3d = recursive_find_python_class([search_in], planner_name3d, current_module=\"nnunet.experiment_planning\")\n",
    "    if planner_3d is None:\n",
    "        raise RuntimeError(\"Could not find the Planner class %s. Make sure it is located somewhere in \"\n",
    "                            \"nnunet.experiment_planning\" % planner_name3d)\n",
    "else: planner_3d = None\n",
    "\n",
    "if planner_name2d is not None:\n",
    "    planner_2d = recursive_find_python_class([search_in], planner_name2d, current_module=\"nnunet.experiment_planning\")\n",
    "    if planner_2d is None:\n",
    "        raise RuntimeError(\"Could not find the Planner class %s. Make sure it is located somewhere in \"\n",
    "                            \"nnunet.experiment_planning\" % planner_name2d)\n",
    "else: planner_2d = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tasks:\n",
    "    print(\"\\n\\n\\n\", t)\n",
    "    cropped_out_dir = os.path.join(nnUNet_cropped_data, t)\n",
    "    preprocessing_output_dir_this_task = os.path.join(preprocessing_output_dir, t)\n",
    "    #splitted_4d_output_dir_task = os.path.join(nnUNet_raw_data, t)\n",
    "    #lists, modalities = create_lists_from_splitted_dataset(splitted_4d_output_dir_task)\n",
    "\n",
    "    ##########################################################################################################\n",
    "    # props_per_case.pkl // intensityproperties.pkl // dataset_properties.pkl 저장\n",
    "    '''\n",
    "    1. props_per_case (nnUNet_cropped_data/Task004_Hippocampus/props_per_case.pkl)\n",
    "        - [patient_identifiers]\n",
    "        - - [has_classes // only_one_region // volume_per_class // region_volume_per_class]\n",
    "    2. intensityproperties (nnUNet_cropped_data/Task004_Hippocampus/intensityproperties.pkl)\n",
    "        - [mod_id]\n",
    "        - -[local_props // median // mean // sd // mn // mx // percentile_00_5 // percentile_99_5]\n",
    "    3. dataset_properties (nnUNet_cropped_data/Task004_Hippocampus/dataset_properties.pkl)\n",
    "        - [all_sizes // all_spacing // segmentation_props_per_patient // class_dct // all_classes]\n",
    "        - [modalities // intensityproperties // size_reductions]\n",
    "    (자세한 tag 정보는 experiment_planning/DatasetAnalyzer.py(analyze_dataset) 참조)\n",
    "    '''\n",
    "    dataset_json = load_json(join(cropped_out_dir, 'dataset.json'))\n",
    "    modalities = list(dataset_json[\"modality\"].values())\n",
    "    collect_intensityproperties = True if ((\"CT\" in modalities) or (\"ct\" in modalities)) else False\n",
    "    dataset_analyzer = DatasetAnalyzer(cropped_out_dir, overwrite=False, num_processes=tf)\n",
    "    _ = dataset_analyzer.analyze_dataset(collect_intensityproperties)\n",
    "    ##########################################################################################################\n",
    "    maybe_mkdir_p(preprocessing_output_dir_this_task)\n",
    "    shutil.copy(join(cropped_out_dir, \"dataset_properties.pkl\"), preprocessing_output_dir_this_task)\n",
    "    shutil.copy(join(nnUNet_raw_data, t, \"dataset.json\"), preprocessing_output_dir_this_task)\n",
    "\n",
    "    threads = (tl, tf)\n",
    "    print(\"number of threads: \", threads, \"\\n\")\n",
    "\n",
    "    \n",
    "    if planner_3d is not None:\n",
    "        exp_planner = planner_3d(cropped_out_dir, preprocessing_output_dir_this_task)\n",
    "        exp_planner.plan_experiment()\n",
    "        if not dont_run_preprocessing:\n",
    "            exp_planner.run_preprocessing(threads)\n",
    "    '''\n",
    "    if planner_2d is not None:\n",
    "        exp_planner = planner_2d(cropped_out_dir, preprocessing_output_dir_this_task)\n",
    "        exp_planner.plan_experiment()\n",
    "        if not dont_run_preprocessing:  # double negative, yooo\n",
    "            exp_planner.run_preprocessing(threads)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nnUNet_plan_and_preprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
